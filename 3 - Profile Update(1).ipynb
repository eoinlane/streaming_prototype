{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "import json\n",
    "import httplib\n",
    "from pykafka import KafkaClient\n",
    "\n",
    "conf = SparkConf().setAppName(\"Sales Order Profile Update\").setMaster(\"spark://server1:7077\")\n",
    "conf.set(\"spark.cores.max\", 3)\n",
    "conf.set(\"spark.cassandra.connection.host\", \"server4\")\n",
    "#conf.set(\"spark.executor.extraClassPath\", \"/root/spark-cassandra-connector-assembly-1.5.0-M1-SNAPSHOT.jar\")\n",
    "#conf.set(\"spark.executor.extraClassPath\", \"/root/spark-streaming-kafka-assembly_2.10-1.5.0.jar\")\n",
    "#conf.set(\"spark.executor.extraClassPath\", \"/root/kafka_2.10-0.8.2.1/libs/kafka_2.10-0.8.2.1.jar\")\n",
    "#conf.set(\"spark.executor.extraClassPath\", \"/root/kafka_2.10-0.8.2.1/libs/zkclient-0.3.jar\")\n",
    "\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Create a local StreamingContext with two working thread and batch interval of 1 second\n",
    "# need change local[2] to spark://server1:7077\n",
    "#sc = SparkContext(\"spark://server1:7077\", \"profile_update\")\n",
    "#sc = SparkContext(\"local[2]\", \"profile_update\")\n",
    "streamingContext = StreamingContext(sc, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "from cassandra.query import BatchStatement\n",
    "\n",
    "class SimpleClient(object):\n",
    "    session = None\n",
    "\n",
    "    def connect(self, nodes):\n",
    "        cluster = Cluster(nodes)\n",
    "        self.session = cluster.connect()\n",
    "        #keyspace is test\n",
    "        self.session.execute(\"use test\")\n",
    "        \n",
    "    def close(self):\n",
    "        self.session.cluster.shutdown()\n",
    "    \n",
    "    def execute(self, statement, parameters=[]):\n",
    "        preparedStatement = self.session.prepare(statement)\n",
    "        return self.session.execute(preparedStatement, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#change localhost:2181 to server7 or zookeeper on server1?\n",
    "stream = KafkaUtils.createStream(streamingContext, 'server1.bigdata.ibm.com:2181', \"profile-update\", {'profile' :1})\n",
    "\n",
    "\n",
    "def processEvent(client, profile):\n",
    "    if(profile.has_key('Score')):\n",
    "        #print '--------update profile----------'\n",
    "        #print profile\n",
    "        client.execute(\"\"\"update customer_product \n",
    "            set LTDSalesAmt = ?, LTDSalesQty = ?, LTDSalesCount = ?, AvgSalesAmt = ?, AvgSalesQty = ?\n",
    "            where customer = ? and product = ? and date = ?\"\"\",\n",
    "                    [profile['LTDSalesAmt'], profile['LTDSalesQty'], profile['LTDSalesCount'], \n",
    "                    profile['AvgSalesAmt'], profile['AvgSalesQty'], \n",
    "                    profile['customer'], profile['product'], profile['date']])\n",
    "    \n",
    "        print '######## update profile:' + str(profile)\n",
    "        return profile\n",
    "    \n",
    "def processPartition(iter):\n",
    "    try:\n",
    "        #cassandra client\n",
    "        client = SimpleClient()\n",
    "        #change hostname to server4 which is the cassandra \n",
    "        client.connect(['server4.bigdata.ibm.com','server5.bigdata.ibm.com','server6.bigdata.ibm.com'])\n",
    "        #client.connect(['sandbox.hortonworks.com'])\n",
    "        #print 'connected'\n",
    "        #kafka producer\n",
    "        #change hostname to server1, which is zookeeper? or server7 which is kafka\n",
    "        #kafkaClient = KafkaClient(hosts=\"server7.bigdata.ibm.com:9092\")\n",
    "        #kafkaClient = KafkaClient(hosts=\"sandbox.hortonworks.com:6667\")\n",
    "        #topic = kafkaClient.topics['profile']\n",
    "        #producer = topic.get_producer()\n",
    "        for record in iter:\n",
    "            #profile = processEvent(client, producer, json.loads(record[1]))\n",
    "            profile = processEvent(client, json.loads(record[1]))\n",
    "    finally:\n",
    "        #print 'disconnected'\n",
    "        client.close()\n",
    "        \n",
    "\n",
    "stream.foreachRDD(lambda rdd: rdd.foreachPartition(processPartition))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "streamingContext.start()             # Start the computation\n",
    "streamingContext.awaitTermination()  # Wait for the computation to terminate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
